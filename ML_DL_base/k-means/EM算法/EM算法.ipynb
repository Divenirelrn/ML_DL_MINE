{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM算法是解决含有隐参数的参数估计问题，如三硬币问题，男女生分布函数求解问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$对于m个互相独立的样本x=(x^{(1)},x^{(2)},...,x^{(m)})，对应的隐含数据 \\\\z = (z^{(1)},z^{(2)},...,z^{(m)})，(x,z)称为完全数据，x为观测数据$<br/>\n",
    "$我们的目标是找到合适的\\theta与z让对数函数极大：$<br/>\n",
    "$$\n",
    "\\theta,z = argmax_{\\theta, z}L(\\theta, z) = argmax_{\\theta, z}\\sum_{i=1}^{m}{log\\sum_{z^{(i)}}{P(x^{(i)},z^{(i)}|\\theta)}}\n",
    "$$\n",
    "$(此处是通过全概率公式得来)$<br/>\n",
    "$则：$<br/>\n",
    "$\\sum_{i=1}^{m}{log\\sum_{z^{(i)}}{P(x^{(i)},z^{(i)}|\\theta)}} = \\sum_{i=1}^{m}{log\\sum_{z^{(i)}}{Q_i(z^{(i)})\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}}}$<br/>\n",
    "$>= \\sum_{i=1}^{m}{\\sum_{z^{(i)}}{Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}}} (2)$<br/>\n",
    "$（此处利用Jensen不等式得来，log(E(y)) >= E(log(y))）$\n",
    "$此处引入一个未知的新的分布Q_i(z^{(i)})，满足：$<br/>\n",
    "$\\sum_{z}{Q_i(z)} = 1, 0 <= Q_i(z) <= 1$<br/>\n",
    "$式（2）即为log\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)}}的期望，即：$<br/>\n",
    "$E(log\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)}}) = \\sum_{z^{(i)}}{Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}}$<br/>\n",
    "$此即为E步中Expection的由来，该期望也被称为Q函数$<br/>\n",
    "$得到不等式，实际上是得到了目标函数的下界，如果使等号成立，则可以得到目标函数\\\\在当前\\theta下的最大下界，即得到在当前\\theta下的最优解。\\\\为使得式(2)中的不等号变为等号，根据Jensen不等式，等号成立的条件是随机变量是常数：$<br/>\n",
    "$$\n",
    "\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})} = c\n",
    "$$\n",
    "$可以推出P(x^{(i)},z^{(i)}|\\theta) = cQ_i(z^{(i)})$<br/>\n",
    "$两边同时累加：\\sum_{z}{P(x^{(i)},z^{(i)}|\\theta)} = c\\sum_{z}{Q_i(z^{(i)})}$<br/>\n",
    "$由于\\sum_{z}{Q_i(z^{(i)})}=1，可得：$<br/>\n",
    "$\\sum_{z}{P(x^{(i)},z^{(i)}|\\theta)} = c$<br/>\n",
    "$则：Q_i(z^{(i)}) = \\frac{P(x^{(i)},z^{(i)}|\\theta)}{c} = \\frac{P(x^{(i)},z^{(i)}|\\theta)}{\\sum_{z}{P(x^{(i)},z^{(i)}|\\theta)}} = \\frac{P(x^{(i)},z^{(i)}|\\theta)}{P(x^{(i)}|\\theta)} = P(z^{(i)}|x^{(i)}, \\theta)$<br/>\n",
    "$即等号成立时，Q(z)为已知样本和模型参数下的隐变量分布$<br/>\n",
    "$至此，我们推出了在已知\\theta下的Q_{i}{(z^{(i)})}选择问题，即推出了期望的形式$<br/>\n",
    "$此时E步完成，进行M步，即最大化似然函数下界：$<br/>\n",
    "$$\n",
    "argmax_{\\theta}{\\sum_{i=1}^{m}{\\sum_{z^{(i)}}{Q_i(z^{(i)})log\\frac{P(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}}}}\n",
    "$$\n",
    "$由于Q_i(z^{(i)})已知(?)，则上式变为：$<br/>\n",
    "$$\n",
    "argmax_{\\theta}{\\sum_{i=1}^{m}{\\sum_{z^{(i)}}{Q_i(z^{(i)})logP(x^{(i)},z^{(i)}|\\theta)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$EM算法流程：$<br/>\n",
    "$1)随机初始化模型参数为\\theta_0$<br/>\n",
    "$2)for j from 1 to J:$<br/>\n",
    "$E步：计算期望（Q函数）：$<br/>\n",
    "$$\n",
    "Q(\\theta, \\theta_i) = \\sum_{z^{(i)}}{Q_i(z^{(i)})logP(x^{(i)},z^{(i)}|\\theta)}\n",
    "$$\n",
    "$M步：极大化Q函数，得到\\theta:$\n",
    "$$\n",
    "argmax_{\\theta}{\\sum_{i=1}^{m}{\\sum_{z^{(i)}}{Q_i(z^{(i)})logP(x^{(i)},z^{(i)}|\\theta)}}}\n",
    "$$\n",
    "$重复E、M步知直到\\theta收敛$<br/>\n",
    "$输出：模型参数\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM算法的收敛性证明：<br/>\n",
    "$证明EM是收敛的，只需证明每一步迭代，都使得似然函数增大，即：$<br/>\n",
    "$\\sum_{i=1}^{m}{logP(x^{(i)}|\\theta^{j+1})} >= \\sum_{i=1}^{m}{logP(x^{(i)}|\\theta^j)}$<br/>\n",
    "$由于：$<br/>\n",
    "$L(\\theta, \\theta^j) = \\sum_{i=1}^{m}{\\sum_{z^{(i)}}{P(z^{(i)}|x^{(i)},\\theta^j)logP(x^{(i)},z^{(i)}|\\theta)}}$<br/>\n",
    "$令H(\\theta,\\theta^j) = \\sum_{i=1}^{m}{\\sum_{z^{(i)}}{P(z^{(i)}|x^{(i)}, \\theta^j)logP(z^{(i)}|x^{(i)}, \\theta)}}$<br/>\n",
    "$两式相减得：\\sum_{i=1}^{m}{logP(x^{(i)}|\\theta)} = L(\\theta, \\theta^j) - H(\\theta, \\theta^j)$<br/>\n",
    "$则\\sum_{i=1}^{m}{logP(x^{(i)}|\\theta^{j+1})} - \\sum_{i=1}^{m}{logP(x^{(i)}|\\theta^j)} = [L(\\theta^{j+1}, \\theta^j) - L(\\theta^j, \\theta^j)] - [H(\\theta^{j+1}, \\theta^j) - H(\\theta^j, \\theta^j)]$<br/>\n",
    "$其中L(\\theta^{j+1}, \\theta^j) - L(\\theta^j, \\theta^j) >= 0, 对于第二部分：$<br/>\n",
    "$H(\\theta^{j+1}, \\theta^j) - H(\\theta^j, \\theta^j) = \\sum_{i=1}^{m}{\\sum_{z{(i)}}{P(z^{(i)}|x^{(i)}, \\theta^j)log\\frac{P(z^{(i)}|x^{(i)}, \\theta^{j+1})}{P(z^{(i)}|x^{(i)}, \\theta^j)}}}$<br/>\n",
    "$<= \\sum_{i=1}^{m}{log(\\sum_{z{(i)}}{P(z^{(i)}|x^{(i)}, \\theta^j)}\\frac{P(z^{(i)}|x^{(i)}, \\theta^{j+1})}{P(z^{(i)}|x^{(i)}, \\theta^j)})}$<br/>\n",
    "$= \\sum_{i=1}^{m}{log(\\sum_{z{(i)}}{P(z^{(i)}|x^{(i)}, \\theta^{j+1})})} = 0$<br/>\n",
    "$因此，\\sum_{i=1}^{m}{logP(x^{(i)}|\\theta^{j+1})} >= \\sum_{i=1}^{m}{logP(x^{(i)}|\\theta^j)}，EM算法一定能够收敛$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM算法一定会收敛，但一般收敛到局部极小值，对于凸函数，可以收敛到全局最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
