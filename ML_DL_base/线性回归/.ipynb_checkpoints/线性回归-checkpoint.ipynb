{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是学习出一条曲线，使训练集上各点到该曲线的距离的平方和最小，且具有较好的泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归模型的一般形式：<br/>\n",
    "$$\n",
    "\\pmb{y} = \\pmb{w^T} \\pmb{x} + \\pmb{b} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss函数为均方误差：<br/>\n",
    "$$\n",
    "loss(y, f(x)) = \\frac{1}{2m}\\sum_{i=1}^{m}{(y-f(x))^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归求解目标函数的方法有两种：<br/>\n",
    "1.最小二乘法：最小二乘法（最小平方法）就是利用计算值与真值之间的平方和误差来计算参数的方法，直接令$\\sum{(y-f(wx + b))^2}$的导数为0，计算出w与b<br/>\n",
    "2.梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是凸优化证明：<br/>\n",
    "loss可以写作：<br/>\n",
    "$$ loss = \\frac{1}{2m} (Y - XW)^T (Y-XW) $$\n",
    "$其中，Y为m \\times 1维，X为m \\times n维，W为n \\times 1维$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则$\\frac{\\partial{loss}}{\\partial{W}}的计算：$<br>\n",
    "$dloss = \\frac{1}{2m} [(d(Y - XW)^T)(Y-XW) + (Y - XW)^Td((Y-XW))] = \\frac{1}{2m} [(d(Y - XW))^T(Y-XW) + (Y - XW)^Td((Y-XW))]=\\frac{1}{2m} [((Y - XW)^Td((Y-XW)))^T + (Y - XW)^Td((Y-XW))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tr(dloss) = dloss = tr(\\frac{1}{2m} [((Y - XW)^Td((Y-XW)))^T + (Y - XW)^Td((Y-XW))]) = \\frac{1}{2m} (tr(((Y - XW)^Td((Y-XW)))^T) + tr((Y - XW)^Td((Y-XW)))) = \\frac{1}{2m} 2tr((Y - XW)^Td(Y-XW)) = \\frac{1}{m} tr((Y - XW)^Td(Y-XW))=-\\frac{1}{m}tr((Y - XW)^TXdW)$<br/>\n",
    "$注意dC = 0, 其中C为常数（常向量、常矩阵）$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则\\frac{\\partial{loss}}{\\partial{W}} = \\frac{1}{m}X^T(XW - Y)(此处利用\\frac{\\partial{loss}}{\\partial{W}}=0即可得最小二乘法的结果)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial^2{loss}}{\\partial{W}\\partial{W^T}}的计算：$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$记g=\\frac{\\partial{loss}}{\\partial{W}} = \\frac{1}{m}X^T(XW - Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则dg = \\frac{1}{m}X^Td(XW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则tr(dg) = dg = \\frac{1}{m}tr(X^Td(XW)) = \\frac{1}{m}tr(X^TXdW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则\\frac{\\partial^2{loss}}{\\partial{W}\\partial{W^T}} = \\frac{1}{m}X^TX,即黑塞矩阵H = \\frac{1}{m}X^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$其二次型x^THx = \\frac{1}{m}x^TX^TXx = \\frac{1}{m}(Xx)^T(Xx)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$向量的内积>=0,因此二次型>=0, H半正定，因此目标函数为凸函数，线性回归为凸优化$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
