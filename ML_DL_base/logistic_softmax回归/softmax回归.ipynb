{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax的决策函数：<br/>\n",
    "$$ h(x_j) = \\frac{e^{wx_j + b}}{\\sum_{i=1}^{n}{e^{wx_i + b}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数的推导（极大似然估计）：<br/>\n",
    "$p(h(x_1), h(x_2), ..., h(x_n)) = p(h(x_1))p(h(x_2))...p(h(x_m)) = \\prod_{i=1}^{m}{p(h(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(h(x_i)) = h(x_i)^{y_i^T}, 其中y_i为标签向量$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则p(h(x_1), h(x_2), ..., h(x_n)) = \\prod_{i=1}^{m}{h(x_i)^{y_i^T}}$<br/>\n",
    "$ln(p(h(x_1), h(x_2), ..., h(x_n))) = \\sum_{i=1}^{m}{y_i^Tln(h(x_i))}$\n",
    "$loss函数为：loss = -\\sum_{i=1}^{m}{y_i^Tln(h(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax回归是凸优化的推导：<br/>\n",
    "$dloss = -\\sum_{i=1}^{m}{y_i^Tdln(h(x_i))} = -\\sum_{i=1}^{m}{y_i^T[d(wx_i) - \\pmb{1}\\frac{1^T(e^{wx_i\\cdot dwx_i})}{1^Te^{wx_i}}]} = -\\sum_{i=1}^{m}{[y_i^Td(wx_i) - \\frac{(e^{wx_i})^Td(wx_i)}{1^Te^{wx_i}}]} = \\sum_{i=1}^{m}{[y_i^T - (s(wx_i))^T]d(wx_i)}$<br/>\n",
    "$这里利用了y^T \\pmb{1} = 1 与 1^T(u\\cdot v) = u^Tv$<br/>\n",
    "$其中s为softmax函数$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则dloss = -\\sum_{i=1}^{m}{tr(x_i(y_i^T - (s(wx_i))^T)dw)} = -\\sum_{i=1}^{m}{tr(x_i(y_i^T - (s(wx_i))^T)dw)}$<br/>\n",
    "$则\\frac{\\partial{loss}}{\\partial{w}} = -\\sum_{i=1}^{m}{(y_i - s(wx_i))x_i^T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$记g = -\\sum_{i=1}^{m}{(y_i - s(wx_i))x_i^T},则：$<br/>\n",
    "$dg = \\sum_{i=1}^{m}{d((s(wx_i) - y_i)x_i^T)}= \\sum_{i=1}^{m}{s{wx_i}x_i^T} = \\sum_{i=1}^{m}{(ds(wx_i))x_i^T} = \\sum_{i=1}^{m}{(s'(wx)dwx_i)x_i^T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dg = \\sum_{i=1}^{m}{tr(x_ix_i^Ts'(wx_i)dw)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$则\\frac{\\partial^2{loss}}{\\partial{w}\\partial{w^T}} = \\sum_{i=1}^{m}{s'(wx_i)x_ix_i^T}>=0, 因此softmax回归为凸优化问题$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
