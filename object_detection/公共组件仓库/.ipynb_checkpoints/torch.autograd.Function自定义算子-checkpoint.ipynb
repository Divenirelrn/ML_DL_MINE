{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch扩展算子（python版本的）一般有两种方式，一是继承自nn.Module，写算子的forward函数，另一种方式是继承torch.autograd.Function，写forward与backward函数。前者可以保存参数，适用于网络层或自定义网络结构，后者不保存参数，适用于relu、pool等操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：Pytorch是利用Variable与Function来构建计算图的。回顾下Variable，Variable就像是计算图中的节点，保存计算结果（包括前向传播的激活值，反向传播的梯度），而Function就像计算图中的边，实现Variable的计算，并输出新的Variable。Function简单说就是对Variable的运算，如加减乘除，relu，pool等。但它不仅仅是简单的运算。与普通Python或者numpy的运算不同，Function是针对计算图，需要计算反向传播的梯度。因此他不仅需要进行该运算（forward过程），还需要保留前向传播的输入（为计算梯度），并支持反向传播计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几个例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-8bedfd045e9f>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8bedfd045e9f>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    ————————————————\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#实现y=wx+b算子\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "# y = w*x + b 的一个前向传播和反向求导\n",
    "class Mul(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b, x_requires_grad = True): # ctx可以理解为元祖，用来存储梯度的中间缓存变量。\n",
    "        ctx.save_for_backward(w,b)     # 因为dy/dx = w; dy/dw = x ; dy/db = 1；为了后续反向传播需要保存中间变量w,x\n",
    "        output = w*x + b\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_outputs):    # 此处grad_outputs 具体问题具体分析\n",
    "        w = ctx.saved_tensors[0]      # 取出ctx中保存的 w = 2\n",
    "        b = ctx.saved_tensors[1]      # 取出ctx中保存的 b = 3\n",
    "        grad_w = grad_outputs * x     # 1 * 1 = 1\n",
    "        grad_x = grad_outputs * w     # 1 * 2 = 2\n",
    "        grad_b = grad_outputs * 1     # 1 * 1 = 1\n",
    "        return grad_w, grad_x, grad_b, None  # 返回的参数和forward的参数一一对应，对于参数x_requires_grad不必求梯度则直接返回None。\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.tensor(1.,requires_grad=True)\n",
    "    w = torch.tensor(2.,requires_grad=True)\n",
    "    b = torch.tensor(3., requires_grad=True)\n",
    "    y = Mul.apply(w,x,b)              # y = w*x + b = 2*1 + 3 = 5\n",
    "    print('forward:', y)\n",
    "    # 写法一\n",
    "    loss = y.sum()                    # 转成标量\n",
    "    loss.backward()                   # 反向传播：因为 loss = sum(y),故grad_outputs = dloss/dy = 1,可以省略不写\n",
    "    print('写法一的梯度：',x.grad, w.grad, b.grad)      # tensor(2.) tensor(1.) tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现sigmoid算子并做梯度检验\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "class Sigmoid(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = 1 / (1 + torch.exp(-x))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "        grad_x = output * (1 - output) * grad_output\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "test_input = torch.randn(4, requires_grad=True)  # tensor([-0.4646, -0.4403,  1.2525, -0.5953], requires_grad=True)\n",
    "print(torch.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=1e-3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
