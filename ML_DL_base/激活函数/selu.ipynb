{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selu(scaled exponential linear units)的数学形式如下：\n",
    "$$\n",
    "f(x) = \\lambda\n",
    " \\begin{cases}\n",
    " x, \\quad if \\quad x>0\\\\\n",
    " \\alpha(exp(x) - 1), \\quad if \\quad x<=0\n",
    " \\end{cases}\n",
    "$$\n",
    "$其中，\\lambda=1.0507, \\alpha=1.6733$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selu的特点：<br/>\n",
    "1)它的提出是为了解决深层的FNN（forward NN, 一般的神经网络）（SNN）无法有效训练的问题<br/>\n",
    "2)自归一化（self-normalizing）：输入分布不设限制，参数的分布假设为N(0,1/K)（lecun_initial）,其中K为参数数量，h=wx+b的分布就为高斯分布，通过SELU激活函数，输出值（激活值）的分布变为N(0,1),即SELU做了一个映射：$g(\\mu, \\nu) \\rightarrow (\\bar \\mu, \\bar \\nu)（\\bar \\mu=0， \\bar \\nu =1）$<br/>\n",
    "3)若要使用dropout，需要使用特制的alphadropout(?)\n",
    "4)对于不接近unit variance的激活值，文章证明有一个upper与lower的bound(?)，因此不可能出现梯度消失或爆炸的问题，可以有效训练深层NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
